---
cover: /articles/article-3/duckgif.gif
date: 2024-05-21
description: Sketch to 2D and 3D with ComfyUI and Stable Diffusion.
layout: article
---
# ComfyUI: Sketch to 2D and 3D

**Transforming sketches into detailed 2D and 3D images using ComfyUI and Stable Diffusion checkpoints.**

![Workflow Screenshot](/articles/article-3/cover.png)
![Workflow Screenshot](/articles/article-3/duckgif.gif)

**ðŸ‘‰ðŸ» Download the workflow - [workflow.json](https://github.com/martintmv-git/comfyui-experiments/blob/main/Sketch%20to%202D%20and%203D/workflow.json).**

## Table of Contents
1. [Models and Nodes Setup](#models-and-nodes-setup)
   - [Models](#models)
   - [Nodes](#nodes)
2. [Workflow Overview](#workflow-overview)
3. [Workflow Results](#workflow-results)


## Models and Nodes Setup

### Models

- [JuggernautXL V9](https://civitai.com/models/133005/juggernaut-xl)
- [SDXL Lightning LoRAs](https://huggingface.co/ByteDance/SDXL-Lightning/tree/main)
- [TripoSR](https://github.com/flowtyone/ComfyUI-Flowty-TripoSR)

### Nodes

- **ComfyUI Manager**: A tool for managing nodes and models within ComfyUI. [ComfyUI Manager GitHub repository](https://github.com/ltdrdata/ComfyUI-Manager)

> **ðŸ‘‰ðŸ» You can install all missing nodes in a workflow, or install custom nodes with ease from the Manager Menu. Restart your ComfyUI after installation.**
![ComfyUI Manager](/articles/article-1/comfyui-manager.png)
Of course, you can install nodes manually with `git` and the `terminal`, but the Manager is a great tool to have.

## Workflow Overview

### 1. Initial Setup and Image Loading
- **Load Image Node**: This is the starting point of the workflow, where an initial image (like a sketch) is loaded into the system. This could be any image you intend to use as a base. 

> It's recommended to use high-quality and square images for better results in the final output, as SDXL models work with square images.

- If you want to draw directly inside ComfyUI, you can use the **PainterNode** to create a sketch. 

> Example: <br>
> Input: ![PainterNode](/articles/article-3/painternode.png) Output: ![PainterNode](/articles/article-3/output-painternode.png)

- **Load Checkpoint Node**: Loads the JuggernautXL V9 checkpoint, providing CLIP, VAE, and others for text and image encoding or decoding. This node ensures that the models necessary for processing and synthesis are ready for the tasks ahead.

- **Load LoRA Node**: Loads the LoRA model, in this specific case - **SDXL Lighting Lora 4 Step**.

![Step 1](/articles/article-3/step1.png)

### 2. Image Preprocessing
- **Image Resize+ Node**: Resizes the loaded image to a suitable resolution (e.g., 1024x1024 pixels), ensuring it meets the input requirements of SDXL for optimal processing and results.

![Step 2](/articles/article-3/step2.png)

### 3. Background Processing
- **RemBG Session+ Node**: Utilizes a background removal model to isolate the subject from its background effectively without any prompt given. 

- **Image Remove Background+ Node**: Refines the removal of the background, working with RemBG and giving `IMAGE` and `MASK` as output.

> This node could be replaced with a more advanced model like a combination of `segment-anything` and `GroundingDINO` that can be prompted to segment a specific element (e.g., "the lighthouse, the boat, etc.") from the image and provide better results.

![Step 3](/articles/article-3/step3.png)

### 4. Textual Conditioning
- **CLIP Text Encode Nodes**: Two instances of this node encode textual descriptions into conditioning signals. 

- The first node handles positive prompts (e.g., "a lighthouse in the middle of the ocean, detailed, photorealistic") and the second handles negative conditioning to avoid unwanted elements (e.g., "cartoonish, low resolution").

![Step 4](/articles/article-3/step4.png)

### 5. Image Synthesis
- **KSampler Node**: A critical node where the actual image synthesis occurs. It takes conditioned inputs and model outputs to generate new sections of the image or modify existing ones, incorporating the learned features and text encodings.

> **Note**: Most of the models and checkpoints you find online have instructions on how to use them with the `KSampler` node as there are specific parameters for each model to perform and give optimal results. 

![Step 5](/articles/article-3/step5.png)

### 6. Latent Space Manipulation
- **VAE Encode Node**: Converts the image into a latent space representation, which is a compressed version that the AI can manipulate more easily.
- **VAE Decode Node**: Transforms the latent representations back into image space, solidifying changes and ensuring that the output remains visually coherent.

![Step 6](/articles/article-3/step6.png)

### 7. Stable Diffusion Outputs
- **Preview Image Nodes**: Used throughout the workflow to visualize the processing stages and final outputs.

![Step 8](/articles/article-3/step8.png)

### 8. 3D Mesh Creation
- **TripoSR Model Loader and Sampler Nodes**: These nodes work together to create and visualize a 3D mesh from the processed image. The TripoSR model is used to generate the 3D mesh, which can be rotated and viewed from different angles.

![Step 7](/articles/article-3/lighthousegif.gif)

## Workflow Results

This `Sketch to 2D and 3D` workflow demonstrates the transformation of simple sketches into detailed 2D and 3D images using ComfyUI. Below are some examples of the final outputs:

> #### Result 1

![Output Result 1](/articles/article-3/output1.png)

![Output Gif 1](/articles/article-3/burgergif.gif)

> #### Result 2

![Output Result 2](/articles/article-3/output2.png)

![Output Gif 2](/articles/article-3/bagofgoldgif.gif)

> #### Result 3

![Output Result 3](/articles/article-3/output3.png)

![Output Gif 3](/articles/article-3/swordgif.gif)

> #### Result 4

![Output Result 4](/articles/article-3/output4.png)

![Output Gif 4](/articles/article-3/lighthouse2.gif)