---
cover: /articles/article-1/cover.png
date: 2024-05-04
description: Stable Diffusion, OpenPose, Depth Anything, and more.
layout: article
---
# ComfyUI: OpenPose and Depth Anything Workflow

**Controlling the generative output of Stable Diffusion by keeping the pose and depth of a subject with a simple ComfyUI workflow.**

![Workflow Screenshot](/articles/article-1/comfyui-openpose.png)

**ðŸ‘‰ðŸ» Download the workflow - [workflow.json](https://github.com/martintmv-git/comfyui-experiments/blob/main/OpenPose%20DepthAnything/workflow.json).**

## Table of Contents
1. [What is ComfyUI?](#but-first-what-is-comfyui)
2. [Models and Nodes Setup](#models-and-nodes-setup)
   - [Models](#models)
   - [Nodes](#nodes)
3. [Workflow Overview](#workflow-overview)
4. [Workflow Results](#workflow-results)


## But first, what is ComfyUI?

**ComfyUI** is a UI that lets you design and execute advanced **Stable Diffusion** pipelines using a **nodes** based interface locally on your computer.

The power of ComfyUI lies in its ability to work effortlessly with ai models, combining different models and building custom workflows that enabling developers to experiment and build with ease.

![What is ComfyUI?](/articles/article-1/comfyui_screenshot.png)

**ðŸ‘‰ðŸ» Learn more at the official [ComfyUI GitHub repository](https://github.com/comfyanonymous/ComfyUI).**

## Models and Nodes Setup

### Models

- [Juggernaut XL](https://civitai.com/models/133005/juggernaut-xl)
- [ControlNet SDXL](https://huggingface.co/lllyasviel/sd_control_collection/tree/main)
- [Depth Anything](https://huggingface.co/LiheYoung/depth_anything_vitl14)


### Nodes

To ensure you have the correct environment setup, you'll need to install the following nodes. These nodes extend the functionality of ComfyUI

- **ComfyUI Manager**: Highly recommended. A tool for managing nodes and models within ComfyUI. [ComfyUI Manager GitHub repository](https://github.com/ltdrdata/ComfyUI-Manager)

> **ðŸ‘‰ðŸ» You can install all missing nodes in a workflow, or install custom nodes with ease from the Manager Menu. Restart your ComfyUI after installation.**
![ComfyUI Manager](/articles/article-1/comfyui-manager.png)
Of course you can install nodes manually with `git` and the `terminal`, but the Manager is a great tool to have.

## Workflow Overview

### 1. Image and Model Loading
- **Load Image**: An image is introduced into the system using a `LoadImage` node.
- **Checkpoint Loader**: A model is loaded with `CheckpointLoaderSimple`, providing CLIP and VAE capabilities for encoding and decoding images and text to and from latent spaces.

![Step 1](/articles/article-1/step1.png)

**ðŸ‘‰ðŸ» You are free to experiment with the model checkpoint, but for the specific workflow I'm using [Juggernaut XL](https://civitai.com/models/133005/juggernaut-xl) from civitai.com**

### 2. Image Processing and Conditioning
- **Image Resize**: The image is resized using `ImageResize+`, preparing it for further processing. Square images are recommended for best results.
- **Openpose Preprocessor**: This node generates pose keypoints from the image, essential for pose analysis.
- **Depth Anything Preprocessor**: Adds depth to the image, enhancing the 3D effect.

![Step 2](/articles/article-1/step2.png)

### 3. Text Encoding
- **CLIP Text Encoding**: Text descriptions are encoded using `CLIPTextEncode` nodes for both positive prompts and negative conditioning, guiding the model on what to include or avoid.

![Step 3](/articles/article-1/step3.png)

### 4. ControlNet Application
- **ControlNet Apply**: Applies specific control networks to adjust the image conditioning based on the stylizations or adjustments defined by the `ControlNetLoader`.

![Step 4](/articles/article-1/step4.png)

### 5. Image Synthesis
- **KSampler**: This node synthesizes new images from the latent representations and conditioning factors, taking inputs from the model and encoded texts.

![Step 5](/articles/article-1/step5.png)

> **Note**: The `KSampler` node is the heart of the workflow. Most of the models and checkpoints you find online have instructions on how to use them with the `KSampler` node as there are specific parameters for each model to perform and give optimal results. 
![Step 5 KSampler](/articles/article-1/step5-ksampler.png)

### 6. VAE Encoding and Decoding
- **VAE Encode and Decode**: The processed image is encoded into a latent space and then decoded back to an image space, finalizing the image modifications.

![Step 6](/articles/article-1/step6.png)

### 7. Image Output
- **Preview Image**: The `PreviewImage` is used at various stages to visualize the processing progress. You can also use the `SaveImage` node to save the final output, but what I like to do is use the `PreviewImage` node and go to the `/temp` folder inside `/ComfyUI` to find the image and save it manually, instead of saving everything.

![Step 7](/articles/article-1/step7.png)

### 8. Upscale and Save
> Optional, but recommended.
- **LDSR Upscaler**: Optionally upscale the image resolution before the final output is displayed. This is useful for high-quality images and prints. There are a lot of methods for upscaling, so this is just one of them.

![Step 8](/articles/article-1/step8.png)

![Final Output](/articles/article-1/output1.png)


## Workflow Results

> #### Result 1

**Input**
- Character from Adobe's [Mixamo](https://www.mixamo.com/).

![Input Screenshot 1](/articles/article-1/input1.png)

**Output**  
![Output Screenshot 1](/articles/article-1/output1.png)

---

> #### Result 2

**Input**  
- Artwork by Anna Kaplan on [ArtStation](https://annetrukhina.artstation.com/).

![Input Screenshot 2](/articles/article-1/input2.png)

**Output**  
![Output Screenshot 2](/articles/article-1/output2.png)

---

> #### Result 3

**Input**  
- Character from Adobe's [Mixamo](https://www.mixamo.com/).

![Input Screenshot 3](/articles/article-1/input3.png)

**Output**  
![Output Screenshot 3](/articles/article-1/output3.png)
